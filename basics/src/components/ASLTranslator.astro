<div class="translator-container">
  <div class="translator-card">
    <h1>Speech to ASL Translator</h1>
    
    <div class="input-section">
      <div class="controls">
        <button id="startRecording" class="button primary">
          <span class="icon">üé§</span>
          Start Recording
        </button>
        <button id="stopRecording" class="button secondary" disabled>
          <span class="icon">‚èπ</span>
          Stop Recording
        </button>
      </div>
      
      <div class="transcript-box">
        <h3>Transcript</h3>
        <div id="transcript" class="transcript-content">
          Your speech will appear here...
        </div>
      </div>
    </div>

    <div class="output-section">
      <h3>ASL Translation</h3>
      <div class="asl-display">
        <div id="aslAnimation" class="asl-animation">
          ASL signs will be displayed here...
        </div>
      </div>
    </div>
  </div>
</div>

<style>
  /* Your existing styles unchanged */
</style>

<script>
  let audioContext;
  let recorder;
  let gumStream;

  const startBtn = document.getElementById("startRecording");
  const stopBtn = document.getElementById("stopRecording");
  const transcriptBox = document.getElementById("transcript");

  // Recorder class (your custom minimal implementation)
  class Recorder {
    constructor(source) {
      this.context = source.context;
      this.node = (this.context.createScriptProcessor || this.context.createJavaScriptNode).call(this.context, 4096, 1, 1);
      this.buffer = [];
      this.recording = false;

      this.node.onaudioprocess = (e) => {
        if (!this.recording) return;
        this.buffer.push(new Float32Array(e.inputBuffer.getChannelData(0)));
      };
      source.connect(this.node);
      this.node.connect(this.context.destination);
    }

    record() {
      this.recording = true;
    }

    stop() {
      this.recording = false;
    }

    exportWAV(cb) {
      let flatBuffer = flattenArray(this.buffer);
      let wavBlob = encodeWAV(flatBuffer, this.context.sampleRate);
      cb(wavBlob);
    }
  }

  function flattenArray(channelBuffer) {
    let length = 0;
    for (let i = 0; i < channelBuffer.length; i++) length += channelBuffer[i].length;
    let result = new Float32Array(length);
    let offset = 0;
    for (let i = 0; i < channelBuffer.length; i++) {
      result.set(channelBuffer[i], offset);
      offset += channelBuffer[i].length;
    }
    return result;
  }

  function encodeWAV(samples, sampleRate) {
    const buffer = new ArrayBuffer(44 + samples.length * 2);
    const view = new DataView(buffer);

    writeString(view, 0, 'RIFF');
    view.setUint32(4, 36 + samples.length * 2, true);
    writeString(view, 8, 'WAVE');

    writeString(view, 12, 'fmt ');
    view.setUint32(16, 16, true);
    view.setUint16(20, 1, true);
    view.setUint16(22, 1, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, sampleRate * 2, true);
    view.setUint16(32, 2, true);
    view.setUint16(34, 16, true);

    writeString(view, 36, 'data');
    view.setUint32(40, samples.length * 2, true);

    floatTo16BitPCM(view, 44, samples);

    return new Blob([view], { type: 'audio/wav' });
  }

  function writeString(view, offset, string) {
    for (let i = 0; i < string.length; i++) {
      view.setUint8(offset + i, string.charCodeAt(i));
    }
  }

  function floatTo16BitPCM(output, offset, input) {
    for (let i = 0; i < input.length; i++, offset += 2) {
      let s = Math.max(-1, Math.min(1, input[i]));
      s = s < 0 ? s * 0x8000 : s * 0x7FFF;
      output.setInt16(offset, s, true);
    }
  }

  startBtn.onclick = async () => {
    try {
      audioContext = new (window.AudioContext || window.webkitAudioContext)();
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      gumStream = stream;

      const input = audioContext.createMediaStreamSource(stream);
      recorder = new Recorder(input);
      recorder.record();

      transcriptBox.textContent = "Recording...";
      startBtn.disabled = true;
      stopBtn.disabled = false;
    } catch (err) {
      transcriptBox.textContent = "Microphone access denied or error.";
      console.error(err);
    }
  };

  stopBtn.onclick = () => {
    if (!recorder) return;

    recorder.stop();
    gumStream.getAudioTracks().forEach(track => track.stop());

    transcriptBox.textContent = "Processing audio...";
    recorder.exportWAV(async (blob) => {
      // Upload WAV blob to backend
      const formData = new FormData();
      formData.append('file', blob, 'audio.wav');

      try {
        const response = await fetch('https://flaskapispeech2text.onrender.com/transcribe', {
          method: 'POST',
          body: formData,
        });

        if (!response.ok) {
          const errorText = await response.text();
          console.error("Server error:", errorText);
          transcriptBox.textContent = `Server error: ${response.status}`;
          return;
        }

        const data = await response.json();
        transcriptBox.textContent = data.transcription || "No speech detected.";
      } catch (err) {
        transcriptBox.textContent = "Error transcribing speech.";
        console.error(err);
      }

      startBtn.disabled = false;
      stopBtn.disabled = true;
    });
  };
</script>